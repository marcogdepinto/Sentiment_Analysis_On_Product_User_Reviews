{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "EgowscHBpR34",
    "outputId": "638f8d6f-373b-4614-b566-c02bc660142e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.9.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.2.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.34.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.29.0)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15) (47.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.0)\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==1.15\n",
    "# @title Preparation\n",
    "!pip install -q keras-bert keras-rectified-adam\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RTP0Zqanppi6",
    "outputId": "6d20bcfb-58f9-4b56-823a-efad6bb1254d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras_radam import RAdam\n",
    "from keras_bert import get_custom_objects\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras_bert import Tokenizer\n",
    "import pandas as pd\n",
    "import tensorflow.keras.backend as K\n",
    "import sys\n",
    "from sklearn.metrics import classification_report\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M30Oo53cqUX2"
   },
   "outputs": [],
   "source": [
    "# @title Constants\n",
    "\n",
    "np.random.seed(42)\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CK2pzzyrnDz"
   },
   "outputs": [],
   "source": [
    "# @title Environment\n",
    "import os\n",
    "pretrained_path = '/content/drive/My Drive/codiesp/alberto_tweets_uncased_L-12_H-768_A-12/'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'model.ckpt-1000000')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocabulary_lower_case_128.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "MZF3iLwpqZGn",
    "outputId": "8c4d0064-e214-4afa-eb7d-637f23894005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From bert_repo/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Load Basic Model\n",
    "import sys\n",
    "\n",
    "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
    "if not 'bert_repo' in sys.path:\n",
    "  sys.path += ['bert_repo']\n",
    "\n",
    "# import python modules defined by BERT\n",
    "from run_classifier import *\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "\n",
    "import codecs\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "colab_type": "code",
    "id": "0BTrr5musRIE",
    "outputId": "721e993c-23f3-4a15-f9c4-48b5715fb0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.18.4)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (0.4.3)\n",
      "Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.0.0)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (5.7)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.41.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.2.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.12.0)\n",
      "Requirement already satisfied: ndjson in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n",
      "WARNING:tensorflow:From bert_repo/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Load Data\n",
    "\n",
    "from keras import Sequential\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "!pip install ndjson\n",
    "import ndjson\n",
    "\n",
    "\n",
    "\n",
    "def _pad(input_ids, max_seq_len):\n",
    "    x = []\n",
    "    input_ids = input_ids[:min(len(input_ids), max_seq_len - 2)]\n",
    "    input_ids = input_ids + [0] * (max_seq_len - len(input_ids))\n",
    "    return np.array(input_ids)\n",
    "\n",
    "#LOADING DATASET\n",
    "#Load the dataset\n",
    "dataframe = pd.DataFrame()\n",
    "\n",
    "with open('ate_absita_training.ndjson') as f:\n",
    "            reader = ndjson.reader(f)\n",
    "\n",
    "            for post in reader:\n",
    "                df = pd.DataFrame([post], columns=post.keys())\n",
    "                dataframe = pd.concat([dataframe, df],\n",
    "                                           axis=0,\n",
    "                                           ignore_index=True)\n",
    "sentences = dataframe['sentence']\n",
    "examples_test = []\n",
    "\n",
    "\n",
    "#Inizialize Text preprocessor\n",
    "text_processor = TextPreProcessor (\n",
    "    # terms that will be normalized\n",
    "    normalize=[ 'url' , 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'] ,\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"} ,\n",
    "    fix_html=True ,  # fix HTML tokens\n",
    "\n",
    "    unpack_hashtags=True ,  # perform word segmentation on hashtags\n",
    "\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts = [ emoticons ]\n",
    ")\n",
    "\n",
    "examples_test = []\n",
    "i = 0\n",
    "for s in sentences:\n",
    "    s = s.lower()\n",
    "    s = str(\" \".join(text_processor.pre_process_doc(s)))\n",
    "    s = re.sub(r\"[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]\", ' ', s)\n",
    "    s = re.sub(r\"\\s+\", ' ', s)\n",
    "    s = re.sub(r'(\\w)\\1{2,}',r'\\1\\1', s)\n",
    "    s = re.sub ( r'^\\s' , '' , s )\n",
    "    s = re.sub ( r'\\s$' , '' , s )\n",
    "    #print(\"Processing:---> \"+s)\n",
    "    examples_test.append(s)\n",
    "    i = i+1\n",
    "\n",
    "#Tokenization\n",
    "#Inizialize the tokenizer\n",
    "tokenizer = tokenization.FullTokenizer(vocab_path, do_lower_case=True)\n",
    "indices_train = []\n",
    "\n",
    "for text in examples_test:\n",
    "  tk = tokenizer.tokenize(text)\n",
    "  tokens = [\"[CLS]\"] + tk + [\"[SEP]\"]\n",
    "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  token_ids = _pad(token_ids,SEQ_LEN)\n",
    "  indices_train.append(token_ids)\n",
    "\n",
    "indices_train = [indices_train, np.zeros_like(indices_train)]\n",
    "\n",
    "train_labels = dataframe['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DFm7TYGvPCBt",
    "outputId": "b48c6ef6-35c3-4f0e-d819-204eb3224c90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3054"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "pMLsCyxc_0UO",
    "outputId": "6b17a131-3cef-4d36-f892-8d1da57a8b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "bert = load_trained_model_from_checkpoint(\n",
    "    config_file=config_path,\n",
    "    checkpoint_file=checkpoint_path,\n",
    "    training=True,\n",
    "    trainable=True,\n",
    "    seq_len=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_S12pUXuj1j"
   },
   "outputs": [],
   "source": [
    "# @title Build Custom Model\n",
    "\n",
    "inputs = bert.inputs[:2]\n",
    "dense = bert.get_layer('NSP-Dense').output\n",
    "dense1 = keras.layers.Dense(units=500, activation='relu') (dense)\n",
    "outputs = keras.layers.Dense(units=1, activation='linear')(dense1)\n",
    "\n",
    "modelk = keras.models.Model(inputs, outputs)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "modelk.compile(\n",
    "    optimizer='adam',\n",
    "    #optimizer='sgd',\n",
    "    loss=root_mean_squared_error,\n",
    "    #loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "    metrics=[root_mean_squared_error]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gep9SctVvQ96"
   },
   "outputs": [],
   "source": [
    "# @title Initialize Variables\n",
    "sess = K.get_session()\n",
    "uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "init_op = tf.variables_initializer(\n",
    "    [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\n",
    ")\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGYsRQRJvVsL"
   },
   "outputs": [],
   "source": [
    "# @title Fit\n",
    "\n",
    "filepath=\"/content/drive/My Drive/codiesp/ate_absita/alberto_sa.{epoch:05d}-{root_mean_squared_error:.5f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='root_mean_squared_error', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#RMSE function\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_train_begin(self, logs=None):\n",
    "    # Initialize the best as infinity.\n",
    "    self.best = np.Inf\n",
    "\n",
    "  def on_epoch_end(self, batch, logs=None):\n",
    "    # @title Predict\n",
    "    predicts = self.model.predict(indices_train, verbose=True)\n",
    "    predictions= []\n",
    "    for a in predicts:\n",
    "      predictions.append(a[0])\n",
    "    rmse_val = rmse(train_labels, predictions)\n",
    "    if rmse_val < self.best:\n",
    "      self.best = rmse_val\n",
    "      self.model.save('alberto_best_model_'+str(rmse_val)+'.h5')\n",
    "    print(\"RMSE is: \"+str(rmse_val))\n",
    "    print('Evaluating: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    MyCustomCallback()\n",
    "]\n",
    "\n",
    "modelk.fit(\n",
    "    indices_train,\n",
    "    train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_2g0uSEOu54"
   },
   "outputs": [],
   "source": [
    "#modelk.save('/content/drive/My Drive/codiesp (1)/ate_absita/alberto_final_model_01.h5')\n",
    "#modelk.load_weights('/content/drive/My Drive/codiesp/ate_absita/alberto_sa_00009-0_95063_1_0334.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jzw09XfMlPY"
   },
   "outputs": [],
   "source": [
    "# @title Predict\n",
    "predicts = modelk.predict(indices_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzXHB5TAIRo_"
   },
   "outputs": [],
   "source": [
    "#RMSE function\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7kwBAlkLtC0"
   },
   "outputs": [],
   "source": [
    "predictions= []\n",
    "for a in predicts:\n",
    "  predictions.append(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muXNFXxxvDI6"
   },
   "outputs": [],
   "source": [
    "rmse_val = rmse(train_labels, predictions)\n",
    "print(\"RMS error is: \" + str(rmse_val))\n",
    "\n",
    "#RMS error is: 1.03338"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "AlBERTo_ATE_ABSITA_SA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
